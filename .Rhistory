library(readr)
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, messages = FALSE)
source('create_example.R')
read_examples()
gc()
tmp <- tokenize_ngrams(df_news, n=1, simplify=FALSE)
library(knitr)
library(readr)
library(dplyr)
library(tidytext)
library(tokenizers)
library(ggplot2)
library(tidyr)
library(scales)
library(stringr)
library(grid)
library(gridExtra)
tmp <- tokenize_ngrams(df_news, n=1, simplify=FALSE)
temp <- unlist(tmp)
temp <- as.data.frame(tmp)
df <- data.frame(matrix(unlist(tmp), nrow=nrow(tmp), byrow=T),stringsAsFactors=FALSE)
df <- data.frame(matrix(unlist(tmp), nrow=nrow(tmp), byrow=T)
)
temp <- do.call(rbind.data.frame, tmp)
View(temp)
?seq
names(temp) <- seq(length(temp))
View(temp)
View(temp)
?zeros
x <- integer(length(temp))
?for
Ã¤l
View(temp)
View(temp)
View(tmp)
length(tmp)
i=5
x[i] <- length(tmp[[i]])
tmp <- tokenize_ngrams(cbin(df_news, df_blogs), n=1, simplify=FALSE)
tmp <- tokenize_ngrams(cbind(df_news, df_blogs), n=1, simplify=FALSE)
tmp <- tokenize_ngrams(cbind(df_news, df_blogs), n=1, simplify=TRUE)
View(tmp)
tmp <- tokenize_ngrams(df_twitter, n=1, simplify=TRUE)
x <- integer(length(tmp))
for(i in seq(length(tmp))){
words_per_line_twitter[i] <- length(tmp[[i]])
}
words_per_line_twitter <- integer(length(tmp))
for(i in seq(length(tmp))){
words_per_line_twitter <- integer(length(tmp))
words_per_line_twitter[i] <- length(tmp[[i]])
}
View(tmp)
tmp <- tokenize_ngrams(df_twitter, n=1, simplify=TRUE)
for(i in seq(length(tmp))){
words_per_line_twitter <- integer(length(tmp))
words_per_line_twitter[i] <- length(tmp[[i]])
}
for(i in seq(length(tmp))){
words_per_line_twitter[i] <- length(tmp[[i]])
}
rm(words_per_line_twitter)
for(i in seq(length(tmp))){
words_per_line_twitter[i] <- length(tmp[[i]])
}
tmp <- tokenize_ngrams(df_twitter, n=1, simplify=TRUE)
words_per_line_twitter <- integer(length(tmp))
for(i in seq(length(tmp))){
words_per_line_twitter[i] <- length(tmp[[i]])
}
df_twitter_1w <- tokenize_ngrams(df_twitter, n=1, simplify=TRUE)
df_blogs_1w <- tokenize_ngrams(df_blogs, n=1, simplify=TRUE)
df_news_1w <- tokenize_ngrams(df_news, n=1, simplify=TRUE)
frequency <- bind_rows(mutate(df_blogs_1w, author = "Blogs"),
mutate(df_news_1w, author = "News"),
mutate(df_twitter_1w, author = "Twitter")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion) %>%
gather(author, proportion, `Blogs`:`News`)
df_blogs_1w <- unlist(df_blogs_1w)
df_news_1w <- unlist(df_news_1w)
df_twitter_1w <- unlist(df_twitter_1w)
frequency <- bind_rows(mutate(df_blogs_1w, author = "Blogs"),
mutate(df_news_1w, author = "News"),
mutate(df_twitter_1w, author = "Twitter")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion) %>%
gather(author, proportion, `Blogs`:`News`)
frequency <- bind_rows(mutate(as.data.frame(df_blogs_1w), author = "Blogs"),
mutate(as.data.frame(df_news_1w), author = "News"),
mutate(as.data.frame(df_twitter_1w), author = "Twitter")) %>%
mutate(word = str_extract(word, "[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion) %>%
gather(author, proportion, `Blogs`:`News`)
frequency <- bind_rows(mutate(as.data.frame(df_blogs_1w), author = "Blogs"),
mutate(as.data.frame(df_news_1w), author = "News"),
mutate(as.data.frame(df_twitter_1w), author = "Twitter")) %>%
mutate(word = str_extract("[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion) %>%
gather(author, proportion, `Blogs`:`News`)
?str_extract
frequency <- bind_rows(mutate(as.data.frame(df_blogs_1w), author = "Blogs"),
mutate(as.data.frame(df_news_1w), author = "News"),
mutate(as.data.frame(df_twitter_1w), author = "Twitter")) %>%
mutate(word = str_extract(1,"[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion) %>%
gather(author, proportion, `Blogs`:`News`)
ggplot(frequency, aes(x = proportion, y = `Twitter`, color = abs(`Twitter` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~author, ncol = 2) +
theme(legend.position="none") +
labs(y = "Twitter", x = NULL)
frequency <- bind_rows(mutate(as.data.frame(df_blogs_1w), author = "Blogs"),
mutate(as.data.frame(df_news_1w), author = "News"),
mutate(as.data.frame(df_twitter_1w), author = "Twitter"))
View(frequency)
frequency <- bind_rows(mutate(data.frame(word = df_blogs_1w), author = "Blogs"),
mutate(data.frame(word = df_news_1w), author = "News"),
mutate(data.frame(word = df_twitter_1w), author = "Twitter"))
View(frequency)
frequency <- bind_rows(mutate(data.frame(word = df_blogs_1w), author = "Blogs"),
mutate(data.frame(word = df_news_1w), author = "News"),
mutate(data.frame(word = df_twitter_1w), author = "Twitter")) %>%
mutate(word = str_extract(word,"[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion) %>%
gather(author, proportion, `Blogs`:`News`)
ggplot(frequency, aes(x = proportion, y = `Twitter`, color = abs(`Twitter` - proportion))) +
geom_abline(color = "gray40", lty = 2) +
geom_jitter(alpha = 0.1, size = 2.5, width = 0.3, height = 0.3) +
geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +
scale_x_log10(labels = percent_format()) +
scale_y_log10(labels = percent_format()) +
scale_color_gradient(limits = c(0, 0.001), low = "darkslategray4", high = "gray75") +
facet_wrap(~author, ncol = 2) +
theme(legend.position="none") +
labs(y = "Twitter", x = NULL)
library(knitr)
library(readr)
library(dplyr)
library(tidytext)
library(tokenizers)
library(ggplot2)
library(tidyr)
library(scales)
library(stringr)
library(grid)
library(gridExtra)
source('create_example.R')
read_examples()
gc()
lines_df_blogs <- length(df_blogs)
lines_df_news <- length(df_news)
lines_df_twitter <- length(df_twitter)
df_twitter_1w <- tokenize_ngrams(df_twitter, n=1, simplify=TRUE)
df_twitter_2w <- unlist(tokenize_ngrams(df_twitter, n=2, simplify=TRUE))
df_twitter_3w <- unlist(tokenize_ngrams(df_twitter, n=3, simplify=TRUE))
df_blogs_1w <- tokenize_ngrams(df_blogs, n=1, simplify=TRUE)
df_blogs_2w <- unlist(tokenize_ngrams(df_blogs, n=2, simplify=TRUE))
df_blogs_3w <- unlist(tokenize_ngrams(df_blogs, n=3, simplify=TRUE))
df_news_1w <- tokenize_ngrams(df_news, n=1, simplify=TRUE)
df_news_2w <- unlist(tokenize_ngrams(df_news, n=2, simplify=TRUE))
df_news_3w <- unlist(tokenize_ngrams(df_news, n=3, simplify=TRUE))
tmp <- tokenize_ngrams(df_blogs, n=1, simplify=TRUE)
words_per_line_blogs <- integer(length(tmp))
for(i in seq(length(tmp))){
words_per_line_blogs[i] <- length(tmp[[i]])
}
tmp <- tokenize_ngrams(df_news, n=1, simplify=TRUE)
words_per_line_news <- integer(length(tmp))
for(i in seq(length(tmp))){
words_per_line_news[i] <- length(tmp[[i]])
}
tmp <- tokenize_ngrams(df_twitter, n=1, simplify=TRUE)
words_per_line_twitter <- integer(length(tmp))
for(i in seq(length(tmp))){
words_per_line_twitter[i] <- length(tmp[[i]])
}
words_df_blogs <- nrow(df_blogs_1w)
words_df_news <- nrow(df_news_1w)
words_df_twitter <- nrow(df_twitter_1w)
word_length_blogs <- mean(nchar(df_blogs_1w))
word_length_news <- mean(nchar(df_news_1w))
word_length_twitter <- mean(nchar(df_twitter_1w))
overview <- data.frame(
Lines = rbind(lines_df_blogs, lines_df_news, lines_df_twitter),
Words = rbind(words_df_blogs, words_df_news, words_df_twitter),
Avrg_Words_per_line = rbind(mean(words_per_line_blogs), mean(words_per_line_news), mean(words_per_line_twitter)),
Avrg_Word_Length = rbind(word_length_blogs, word_length_news, word_length_twitter)
)
mean(words_per_line_blogs)
rbind(mean(words_per_line_blogs), mean(words_per_line_news), mean(words_per_line_twitter))
rbind(word_length_blogs, word_length_news, word_length_twitter)
rbind(words_df_blogs, words_df_news, words_df_twitter)
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, messages = FALSE)
library(knitr)
library(readr)
library(dplyr)
library(tidytext)
library(tokenizers)
library(ggplot2)
library(tidyr)
library(scales)
library(stringr)
library(grid)
library(gridExtra)
source('create_example.R')
read_examples()
gc()
lines_df_blogs <- length(df_blogs)
lines_df_news <- length(df_news)
lines_df_twitter <- length(df_twitter)
df_twitter_1w <- tokenize_ngrams(df_twitter, n=1, simplify=TRUE)
df_twitter_2w <- unlist(tokenize_ngrams(df_twitter, n=2, simplify=TRUE))
df_twitter_3w <- unlist(tokenize_ngrams(df_twitter, n=3, simplify=TRUE))
df_blogs_1w <- tokenize_ngrams(df_blogs, n=1, simplify=TRUE)
df_blogs_2w <- unlist(tokenize_ngrams(df_blogs, n=2, simplify=TRUE))
df_blogs_3w <- unlist(tokenize_ngrams(df_blogs, n=3, simplify=TRUE))
df_news_1w <- tokenize_ngrams(df_news, n=1, simplify=TRUE)
df_news_2w <- unlist(tokenize_ngrams(df_news, n=2, simplify=TRUE))
df_news_3w <- unlist(tokenize_ngrams(df_news, n=3, simplify=TRUE))
tmp <- tokenize_ngrams(df_blogs, n=1, simplify=TRUE)
words_per_line_blogs <- integer(length(tmp))
for(i in seq(length(tmp))){
words_per_line_blogs[i] <- length(tmp[[i]])
}
tmp <- tokenize_ngrams(df_news, n=1, simplify=TRUE)
words_per_line_news <- integer(length(tmp))
for(i in seq(length(tmp))){
words_per_line_news[i] <- length(tmp[[i]])
}
tmp <- tokenize_ngrams(df_twitter, n=1, simplify=TRUE)
words_per_line_twitter <- integer(length(tmp))
for(i in seq(length(tmp))){
words_per_line_twitter[i] <- length(tmp[[i]])
}
df_blogs_1w <- unlist(df_blogs_1w)
df_news_1w <- unlist(df_news_1w)
df_twitter_1w <- unlist(df_twitter_1w)
words_df_blogs <- nrow(df_blogs_1w)
words_df_news <- nrow(df_news_1w)
words_df_twitter <- nrow(df_twitter_1w)
word_length_blogs <- mean(nchar(df_blogs_1w))
word_length_news <- mean(nchar(df_news_1w))
word_length_twitter <- mean(nchar(df_twitter_1w))
rm(tmp)
rm(tmp, df_blogs, df_news, df_twitter)
words_df_blogs <- length(df_blogs_1w)
words_df_news <- length(df_news_1w)
words_df_twitter <- length(df_twitter_1w)
overview <- data.frame(
Lines = rbind(lines_df_blogs, lines_df_news, lines_df_twitter),
Words = rbind(words_df_blogs, words_df_news, words_df_twitter),
Avrg_Words_per_line = rbind(mean(words_per_line_blogs), mean(words_per_line_news), mean(words_per_line_twitter)),
Avrg_Word_Length = rbind(word_length_blogs, word_length_news, word_length_twitter)
)
row.names(overview) <- c("Blogs", "News", "Twitter")
rm(lines_df_blogs, lines_df_news, lines_df_twitter,
words_df_blogs, words_df_news, words_df_twitter,
words_per_line_blogs, words_per_line_news, words_per_line_twitter,
word_length_blogs, word_length_news, word_length_twitter
)
frequency <- bind_rows(mutate(data.frame(word = df_blogs_1w), author = "Blogs"),
mutate(data.frame(word = df_news_1w), author = "News"),
mutate(data.frame(word = df_twitter_1w), author = "Twitter")) %>%
mutate(word = str_extract(word,"[a-z']+")) %>%
count(author, word) %>%
group_by(author) %>%
mutate(proportion = n / sum(n)) %>%
select(-n) %>%
spread(author, proportion) %>%
gather(author, proportion, `Blogs`:`News`)
View(frequency)
gc()
knitr::opts_chunk$set(echo = FALSE, warning=FALSE, messages = FALSE)
library(knitr)
library(readr)
library(dplyr)
library(tidytext)
library(tokenizers)
library(ggplot2)
library(tidyr)
library(scales)
library(stringr)
library(grid)
library(gridExtra)
source('create_example.R')
read_files()
library(knitr)
library(readr)
library(dplyr)
library(tidytext)
library(tokenizers)
library(ggplot2)
library(tidyr)
library(scales)
library(stringr)
library(grid)
library(gridExtra)
source('create_example.R')
read_files()
gc()
lines_df_blogs <- length(df_blogs)
lines_df_news <- length(df_news)
lines_df_twitter <- length(df_twitter)
df_blogs_1w <- tokenize_ngrams(df_blogs, n=1, simplify=TRUE)
df_blogs_2w <- unlist(tokenize_ngrams(df_blogs, n=2, simplify=TRUE))
df_blogs_3w <- unlist(tokenize_ngrams(df_blogs, n=3, simplify=TRUE))
rm(df_blogs)
words_per_line_blogs <- integer(length(df_blogs_1w))
for(i in seq(length(df_blogs_1w))){
words_per_line_blogs[i] <- length(df_blogs_1w[[i]])
}
df_blogs_1w <- unlist(df_blogs_1w)
words_df_blogs <- length(df_blogs_1w)
word_length_blogs <- mean(nchar(df_blogs_1w))
Blogs <- c(lines_df_blogs, words_df_blogs, mean(words_per_line_blogs), word_length_blogs)
words_per_line_blogs <- integer(length(df_blogs_1w))
for(i in seq(length(df_blogs_1w))){
words_per_line_blogs[i] <- length(df_blogs_1w[[i]])
}
df_blogs_1w <- unlist(df_blogs_1w)
words_df_blogs <- length(df_blogs_1w)
word_length_blogs <- mean(nchar(df_blogs_1w))
Blogs <- c(lines_df_blogs, words_df_blogs, mean(words_per_line_blogs), word_length_blogs)
lines_df_news <- length(df_news)
df_news_1w <- tokenize_ngrams(df_news, n=1, simplify=TRUE)
df_news_2w <- unlist(tokenize_ngrams(df_news, n=2, simplify=TRUE))
df_news_3w <- unlist(tokenize_ngrams(df_news, n=3, simplify=TRUE))
words_per_line_news <- integer(length(df_news_1w))
for(i in seq(length(df_news_1w))){
words_per_line_news[i] <- length(df_news_1w[[i]])
}
df_news_1w <- unlist(df_news_1w)
words_df_news <- length(df_news_1w)
?sample
?tokenize_ngrams
library(knitr)
library(readr)
library(dplyr)
library(tidytext)
library(tokenizers)
library(ggplot2)
library(tidyr)
library(scales)
library(stringr)
library(grid)
library(gridExtra)
?okenize_ngrams
?tokenize_ngrams
#Empty Environment
rm(list=ls())
cat("\f")
#Packages
library(readr)
library(dplyr)
library(tidytext)
library(ggplot2)
library(tidyr)
library(scales)
library(stringr)
#Testdata
source('create_example.R')
read_examples()
df_blogs <- data_frame(line = 1:length(df_blogs), text = df_blogs)
df_news <- data_frame(line = 1:length(df_news), text = df_news)
df_twitter <- data_frame(line = 1:length(df_twitter), text = df_twitter)
View(df_blogs)
df_blogs <- df_blogs %>% unnest_tokens(word, text) %>% anti_join(stop_words)
df_news <- df_news %>% unnest_tokens(word, text) %>% anti_join(stop_words)
df_twitter <- df_twitter %>% unnest_tokens(word, text) %>% anti_join(stop_words)
View(df_blogs)
library(tm)
library(textmineR)
library(tokenizers)
my_tokenizer <- NgramTokenizer(min=2, max=3)
df_twitter_2w <- tm::DocumentTermMatrix(corp, df_twitter=list(tokenize=my_tokenizer))
df_twitter_2w <- MakeSparseDTM(df_twitter_2w)
install.packages('tm')
install.packages('tm')
install.packages('textmineR')
install.packages('tokenizers')
install.packages("tokenizers")
library(tm)
library(textmineR)
library(tokenizers)
my_tokenizer <- NgramTokenizer(min=2, max=3)
df_twitter_2w <- tm::DocumentTermMatrix(corp, df_twitter=list(tokenize=my_tokenizer))
df_twitter_2w <- MakeSparseDTM(df_twitter_2w)
??NgramTokenizer
install.packages('tokenizers')
install.packages("tokenizers")
df_twitter_2w <- tm::DocumentTermMatrix(corp, df_twitter=list(tokenize=my_tokenizer))
install.packages('textmineR')
install.packages("textmineR")
my_tokenizer <- textmineR::NgramTokenizer(min=2, max=3)
my_tokenizer <- NgramTokenizer(min=2, max=3)
?
NgramTokenizer
??NgramTokenizer
my_tokenizer <- textmineR::NgramTokenizer(min=2, max=3)
my_tokenizer <- textmineR::NgramTokenizer(min=2, max=3)
df_twitter_2w <- tm::DocumentTermMatrix(corp, df_twitter=list(tokenize=my_tokenizer))
df_twitter_2w <- MakeSparseDTM(df_twitter_2w)
